##Theory and Methodology

- Need to update to reflect new work with byzortho
- Need to include the work which allows for the citation of text versions



--------------------





---------------------

Even if the content of this thesis seems to focus strongly on the analysis of computer-generated data, the question which provides the basis for this thesis is one which has perplexed scholars of the Venetus A for centuries. As explained in the previous chapter, there has been no consensus on how to interpret the unique five-zoned layout of the Venetus A scholia. As far back as 1875, the prominent Homeric scholar Karl Dindorf made a number of significant conclusions about different aspects of the scholia, but according to modern standards, his methodology undermines the strength of many of his conclusions. In his whole discussion of the nature of intermarginal and main scholia, he only mentions a handful of examples. In essence, he requires his readers to rely just on his ethos as a scholar to support his final conclusion. This is not to say that Dindorf's conclusions are incorrect, nor is it fair to say that Dindorf was an inferior scholar and conducting poor scholarship. It may be exceedingly possible that Dindorf had a wealth of examples to support his conclusion, but a print format prevented Dindorf from filling his introduction with too many examples.

However, even if Dindorf had included every single piece of evidence he had collected to support his conclusions, his methodology would still put his findings under suspicion. Dindorf and nearly every other scholar prior to the rise of digital technologies relied on close-reading for any and all textual analyses. As Matthew Jockers notes in his book on the method of "distant-reading," close-reading is best applied to either individual texts or a small subset of them (19). Thus, something like a single poem lends itself well to a close-reading, but the entire corpus of British novels does not. It follows, then, that the estimated ten thousand scholia of the Venetus A manuscript constitute too large of a corpus for close-reading to be effective. Considering also that the scholia are an incredibly diverse set of documents, it would be unlikely that Dindorf or any other scholar could make many intelligible comments about minute linguistic patterns over so large and varied a corpus. Therefore, criticizing Dindorf or most of the previous Venetus A scholars is not quite fair, since they did not have the tools to analyze the text in a more systematic fashion.

Simply put, a computer is required for the "distant-reading" which the Venetus A scholia demands. Jockers defines this "distant-reading" as a sort of macroanalysis that, while devoid of "human synthesis and intuition," can process far more information and in much less time (19). Jockers' book details many examples of valid scholarly questions about literature that simply cannot be answered using close-reading alone. Typically, his examples deal with massive amounts of data, such as the entire corpus of nineteenth-century Irish literature. However, Jockers is by no means advocating for the abolition of close-reading in favor of his "distant-reading." Instead, he compares the relationship between close and distant reading to that of micro and macro economics. He writes, "Just as microeconomics offers important perspectives on the economy, so too does close-reading offer fundamentally important insights about literature" (26). Ultimately, he concludes, mining data from macroanalysis is not enough, but "human interpretation of the data... remains essential" (26). Thus, distant-reading is meant to complement close-reading, not replace it.

The number of avenues for pursuing such distant-reading continues to grow as the field of digital humanities itself continues to grow. A fuller description of the technologies applied in this thesis will be put forth later when it becomes more relevant. Still, it is worth mentioning the two primary digital methods: topic models and embedded word-vectors. 

Here, however, it is necessary to discuss the first step in any research where textual analysis is heavily reliant upon digital technologies. One must make the text or texts of interest machine-readable before any analysis can be carried out. For example, a concatenation of every non-white space character in the first line of Edgar Allen Poe's "The Raven" can be read by a human, albeit with some difficulty. On the other hand, this string of characters, "`Onceuponamidnightdreary,whileIpondered,weakandweary,`", is read by a computer as just that: a seemingly random collection of characters. Relying on digital techonologies to identify the meaningful word breaks in this example would certainly prove difficult, rendering the above string essentially useless for any textual analysis. While it seems unlikely that one might ever encounter such a concatenated text, texts are often transformed in unexpected ways when one collects them for analysis from various webpages and outside sources. Scholars are not usually interested in analyzing texts which they themselves have created, but rather those which are available elsewhere, such as on an online archive. Thus importing an already-fashioned text requires a fair amount of careful prep work, or text wrangling, in order for it to suit one's analysis.

Still, even the line `Once upon a midnight dreary, while I pondered, weak and weary,` would not be ready for most forms of textual analysis. It is true that the above text could be more easily broken into individual words and analyzed by a computer. While one might recognize a list of individual words `Once` `upon` `a` `midnight` etc. as being the opening words of Poe's poem, a computer will not. To the computer it is just a list of seemignly random words. What is needed is some sort of metadata which is connected to the text that allows the computer to recognize that "Once upon a midnight dreary..." should not be considered equivalent to "Two roads diverged in a yellow wood." In other words, in order to create machine-readable texts, the text has to be cited.

The simplest way of creating a citable text of a work is to prepare a text in two columns. The first column carries a citation, the second column carries the text. The depth of one's citation depends heavily on what sort of analysis is required. It is reasonable that if one wanted to compare the entire corpus of Robert Frost poems, one would have the first column contain a citation of a particular poem and the second column contain the entire poem. More commonly though, scholars like to refer to chapters of a book, or line of a poem. Thus a citable text version of the first two lines of Frost's "The Road Not Taken" might look something like:

| Citation | Text |
| --- | --- |
| urn:EnglishLit:Poetry:American:Robfrost:poem1.poetryFound:1 | Two roads diverged in a yellow wood. |
| urn:EnglishLit:Poetry:American:Robfrost:poem1.poetryFound:2 | And sorry I could not travel both |
 
While the first two lines of "Stopping by Woods on a Snowy Evening" might look like:

| Citation | Text |
| --- | --- |
| urn:EnglishLit:Poetry:American:Robfrost:poem2.poetryFound:1 | Whose woods these are I think I know.
| urn:EnglishLit:Poetry:American:Robfrost:poem2.poetryFound:2 | His house is in the village though;

While this Robert Frost citation scheme is merely hypothetical and not actually in use anywhere, my example is based upon an actual model for creating citable texts: the Ordered Hierarchy of Citation Objects (OHCO2). The above example demostrates the prominent features of the OHCO2 model. The model defines a text as "a set of citable nodes," in this case a line of poetry. Each line of the poem is situated within a larger citation scheme that allows it to be distinguised from a line of poetry in some other work. As can be gleaned from the above example, the two citations of the two Frost poems are nearly identical except for one being "poem1" and the other being "poem2". The designations of 1 and 2 are arbitrary, but they have to be consistent. When discussing the poems of Frost, "poem1" must always be referring to "The Road Not Taken". Furthermore, there was no need for me to use numbers in my citation - they could have easily been "poemA" and "poemB". Again, as long as the citation scheme is unambiguous and consistent, there are no issues. 

Moving beyond the "poem1" and "poem2", it is worth noting the other features of this citation hierarchy. Specifically these two poems belong under the classification of "RobFrost" conveying that the two poems are clustered together because Frost penned them both. The citation `urn:EnglishLit:Poetry:American:Robfrost` would be an appropriate way to cite all the poems of Robert Frost. "RobFrost" in turn belongs under the hierarchy of "American" indicating that Frost wrote American works and, more specifically since "American" falls under the hierarchy of "Poetry" that he wrote American poetry. Finally, most generally these two poems fall under the heading of "EnglishLit". Thus the OHCO2 citation model allows for the citation of a work within the context of the rest of literature. One could imagine various permutations to the schema aboe to account for different types of literature. urn:EnglishLit:Poetry:American:EAPoe would allow for the citation of Edgar Allen Poe's poetry, urn:EnglishLit:Poetry:Irish would allow for the citation of Irish poetry, and urn:LatinLit would allow for the citation of Latin literature.

Directly after "poem1" and "poem2" came the ciation "poetryFound". This refers to the work hierarchy of a text, i.e. what specific source provided the text one is citing. In this case, I found the text for the two Frost poems on the website *Poetry Foundation*, thus `poetryFound` is meant to reflect my direct source. The distinction here is ultimately to what the developers of the OHCO2 model refers to as the "notational text" and one particular version of a text. Whereas the citation urn:EnglishLit:Poetry:American:Robfrost:poem1 refers to the abstract concept that is Frost's "The Road Not Taken", urn:EnglishLit:Poetry:American:Robfrost:poem1.poetryFound refers to a version of the text that exists on a particular website, as opposed to one found in a print anthology.

After "poetryFound" comes the citation which differentiates the individual lines from one another. This is the final bit of information within conveyed by this identifier with the OHCO2 model. Simply stated, most books and poems have some sort of hierarchy which dictate in what order the lines or sentences are to be read. Thus there is a first line, followed by the second, followed by the third, and so on. This form of citation should be familiar to anyone who has read Shakespeare or most Classical poetry, where each line is given a unique identifier. No matter what version of the text one holds, there will always a exist a line1 and line2 and line3 and so on. 

The "urn" which begins each of the citations does not carry any information about the text, rather it is indicating that the citation is following the format of the Uniform Resource Name (urn) scheme. 

So in sum, the OHCO2 citation model requires a citation which situations a particular text within the broader spectrum of genre and work classification, similar to how the Dewey Decimal system works. Additionally, the citation must provide information about which particular version of a work one is dealing with, whether a specific print edition or just the more abstract notional text. Finally, it must give some information of the internal structure of the work itself, whether that means book chapters, paragraphs, sentences, lines of poetry, etc. 

Citing texts in such a systematic and orderly fashion is extremely useful for a number of reasons, but it is particularly useful as it allows scholars to have a pre-designed method for referring to passages of a text unambiguously. Different editions of a book may have different page numbers, but we presume that it will have the same chapters and paragraphs. In other words the citation scheme is not dependent on any outside device like a page number. urn:EnglishLit:Poetry:American:Robfrost:poem1:1 would represent the first line of "The Road Not Taken" irregardless if one person is reading the text on a computer screen and another in a print anthology. Furthermore, the citation can be understood by a computer as well, given that a program is written to make sense of urn citations. 

However, to reiterate, all the example citations given so far have been just that, examples. They work well in this context of explaining the importance of citing a text, but a computer without any prior programming would be just as confused as an English professor if it were expected to know that urn:EnglishLit:Poetry:American:Robfrost:poem1 was a citation for "The Road Not Taken." In other words, in order to have a meaningful citation, there needs to be an agreed upon standard of work classification, like the Dewey Decimal system. Luckily, for the field of Classical studies, such an ordered index of all Greek has already been put together by the Thesaurus Linguae Graecae, an library for ancient Greek literature. Thus, a citation like urn:cts:greekLit:tlg0012.tlg001 actually means something beyond an individual project. In this citation `urn:cts:` is still just functioning to define what type of citation scheme is being followed. The node `greekLit` is making reference to a specific index of works, in this case the index of Greek works put together by the Center for Hellenic Studies (http://homermultitext.github.io/hmt-docs/faq/). The node `tlg0012.tlg001` is the aspect that makes use of the  index previously put together by the Thesaurus Linguae Graecae (tlg). `tlg0012` is the index number for Homeric poetry, and with the addition of `tlg001`, `tlg0012.tlg001` specifically refers to the *Iliad*. Again, urn citations do not have to link up with outside indices, and can simply be project specific. As long as the conventions of the citation are employed consistently and do not lead to ambiguous citations, the citations are valid. However, it can often be easier when trying to make a citation scheme more understandable to those outside of a specific project if the citation can be related back to an already existing index, such as the index created by the Thesaurus Linguae Graecae.

If one wants to apply a rather strict citation scheme upon a text, it is best if the text actually fit well within that citation scheme. So for example, if you want to cite individual lines of poetry, it is important that the text actually be broken out into lines. 

Luckily, in the case of this work on Iliadic scholia, the text of the scholia has already been written in a format that easily lends itself to citation as well as digital analysis. This is because, as mentioned previously in chapter 1, the Homer Multitext project has been working for the last nine years to create digital editions of entire Iliad manuscripts. The scholia to the Venetus A manuscript have been transcribed by members of the project using an XML format. Each book of scholia, along with each type of scholia is transcribed into its own separate edition. By writing in XML, or eXtensible Markup Language, the editors of the Homer Multitext project are able not only to record the visible letters on the manuscript page, but also to apply "markup" in order to supply more information about the text. Specifically, the Homer Multitext project follows the guidelines of the Text Encoding Initiative (TEI), a community of digital humanists who have established standards for editing texts. These guidelines inform the project's policies on creating digital editions such that the markup can be understood easily by other scholars. This extra information can convey how the scholia are ordered on the page, to which line of the poem a scholion refers, and whether a string of text in the scholion is a quotation from elsewhere in the *Iliad*. These and other more specific TEI elements will be explained as they become relevant. However, this section will focus on the structural aspects of the TEI guidelines as they relate to text wrangling. 

For example, above it was said that it is important that a poem be broken up into lines if one wanted to cite the poem as a set of lines. In XML, there is an element `<l>` which means "line," so one could just transcribe a poem into an XML format where each line is contained within its own `<l>` element. Additionally, one can add attributes to the element in order to add more information about the element. So for the element `<l>`, one of the attributes is `@n`, meaning "name". This `@n` attribute can be used to give some sort of identifier to a line. So logically, the first line of a poem might have the identifier `1`. So in total the first line of "The Road Not Taken" in XML might look something like: `<l n="1">Two roads diverged in a yellow wood.<\l>`

When it comes to creating an XML edition of the scholia, the Homer Multitext project uses the same basic structural format for every scholion. That is, within one TEI element `<div>`, "division", there are three parallel subdivisions, still the element `<div>`, which each carry information about the scholion. The first subdivision contains just the lemma of the scholion. The lemma is a quotation of a whole of part of an *Iliad* line which appears at the beginning of a scholion and serves to clue the reader into which line of the *Iliad* the scholion is going to comment on. The second subdivision contains a URN citation for the line of the *Iliad* on which the scholion is commenting, and the final subdivision contains the actual text content of the scholion itself. Such a logical structure is essential for the creation of a digital scholarly edition.

So below is an edited scholion commenting on *Iliad* 18, line 4 with its complete XML mark-up.

`<div n="4" type="scholion">
   <div n="lemma">
      <p>τα φρονέοντ' ἀνα θυμὸν,</p>
   </div>
   <div type="ref">
      <p>urn:cts:greekLit:tlg0012.tlg001.msA:18.4</p>
   </div>
   <div n="comment">
     <p>οἱ περι τῶν φαύλων αγωνιῶντες ἐν τοῖς κινδύνοις δυσέλπιδές εἰσὶν ἔστι
        δὲ τῶν ἐν ατυχίᾳ προληπτικῶς ὁ νοῦς ⁑</p>
   </div>
</div>`

While the XML mark-up helps keep a text structured and organized, this additional structural information hinders textual analysis. While something like `<div type="text"><l>Wrath sing, oh goddess, of son of Peleus Achilles</l></div>` is a valid and logical XML transcription of the first line of the *Iliad*, clearly text like "<div>" and "<l>" should not be included if one wanted to analyze the number of characters are in the first line of the poem. If one were working with a small amount of scholia, it may make sense just to manually copy and paste all of the actual text out of the XML mark-up. The 8,000 scholia of this dataset, however, render manual extraction impractical. In order to avoid this labor-intensive process, we wrote a program using scala, a computer programming language, which was able to perform this extraction automatically.

Even with a complete collection of only the text characters words from all the scholia, such a dataset would still be insufficient for any serious textual analysis if left unaltered. The Homer Multitext creates diplomatic editions of manuscripts, meaning that every intentional mark is transcribed exactly as it appears on the manuscript page. This means that even if a scribe makes a spellingerror that would be easily to fix, the manuscript is transribed in such a way that the error is retained. Thus, the various orthographic variants and scribal errors cluttering up their data. Even the editing process of the HMT can leave behind leftover that clutter the data. Abbreviations serve as a good example of this principle. Very often in the scholia, the scribe will choose to abbreviate words. With some experience reading the manuscript, these abbreviations are fairly easy to understand. The string of characters, Ἀρίσταρχ, surely must be an abbreivation for Ἀρίσταρχος (Aristarchus), the Alexandrian editor. The HMT does nt want to ignore the fact that the name was orignially abbreviated, which would invaldiate its claim that it is creating a diplomatic edition. However, refusing to expand the abbreviation would make the project's editions difficult to use in text analysis. This is because Ἀρίσταρχ does not have to expand to Ἀρίσταρχος, which is the nominative form of Aristarchus' name. Greek is an inflected language, meaning that endings of words change depending on the words function within a sentence. So Ἀρίσταρχ could also expand into  Ἀριστάρχου, Ἀριστάρχῳ, or Ἀρίσταρχον depending on how Aristarchus is being used in the sentence. Thus, just knowing that Ἀρίσταρχ appears in the text would not allow for a complete analysis of the sentence structure, since the ending of Ἀρίσταρχ would be unknown.

The HMT has reached a compromise on this issue; they record both the abbreviated and expanded form of word in their XML edition, by utilizing the TEI element `<choice>` which allows two versions of the same word to be recorded. So in the example of Ἀρίσταρχ, it would be marked-up like this: 

`<choice><abbr>Ἀρίσταρχ</abbr><expan>Ἀρίσταρχος</expan></choice>` 

While the above mark-up allows for the editors to make a compromise between presenting a purely diplomatic edition, and a normalized, more easily readble edition, it poses a challenge for textual analysis. As can clearly be seen, if one were to simply extract all the text from the XML mark-up, every instance of `<choice><abbr>Ἀρίσταρχ</abbr><expan>Ἀρίσταρχος</expan></choice>` would yield both Ἀρίσταρχ and Ἀρίσταρχος. This poses a problem for analysis, obviously because now there are two words to represent one. If one wanted to know how often the string of characters Ἀρίστ appeared in the scholia, the result would be double what the reality is. 

For this reason, and others which will be explained below, it became necessary to rethink the approach to analyzing the text. Rather than just extracting all the text at once from the XML, it became necessary to create different versions of the text according to different standards of reading the text. For example, if one wanted a truly diplomatic edition, one where none of the abbreviations are expanded, then a text should be created wherein the XML-extracting script ignores every `<expan>` element. Similarly, if someone were interested in a more normalized text where every abbreviation *is* expanded, the XML-extracting script should ignore every `<abbr>` element. This same logic applied to instances where a scribe originally wrote one thing, but later corrected it. There should be a version of the text which includes only these original errors, and then a separate edition which records only the corrections of the scribes. This is exactly what was done. There are currently two separate editions of the scholia, one which selects the more diplomatic option at every `<choice>` element, and one which selects the more normalized option at every `<choice>` element.

Wha is important to note that these two separate versions of the scholia is that both the diplomatic and normalized versions can be cited within the already existing OHCO2 citation scheme for the scholia. Recalling the above discussion, the diplomatic and normalized versions would simply fall underneath the general work hierarchy. Just as in the example URN urn:EnglishLit:Poetry:American:Robfrost:poem1.poetryFound, `poetryFound` referred to the fact that this version of "The Road Not Taken" was from the website PoetryFoundation, the URN for the scholia could simply add `diplomatic` or `normalized` in order to specify which version of the scholia is being worked with. So urn:cts:greekLit:tlg5026.msA.hmt.diplomatic:1.1 and urn:cts:greekLit:tlg5026.msA.hmt.normalized:1.1 would both constitute a valid citation for the same scholion. Just as before `urn:cts:` simply give information about the structure of the citation, and `greekLit:` refers a specific index of Greek works collected by the Center for Hellenic studies. `tlg5026` represents the index number given to Iliadic scholia by the Thesaurus Linguae Graecae, while `msA.hmt` specifies that within the vast range of Iliadic scholia, this citation is dealing only with the Venets A (msA), and only a version prepared by the Homer Multitext (hmt). Finally, this particular scholion is the first scholion of the first book of the Iliad (1.1). The only thing that differs is in the work hierarchy with either "diplomatic" or "normalized" appearing. By treating the two versions of the text as their own separate editions, any subsequent analyses on the text of the scholia is entirely separate from the transcription put forth by the Homer Multitext project. That is, any arguments about the text can be made with clear citations to the exact version of the text being used (diplomatic, normalized, etc.). If this were not the case, the only existing citation would be the standard citation for the Homer Multitext's edition. However, the literal string of the HMT edition was shown to impede logical text analysis, and would have to be altered. These version citations allow these necessary alterations to be plainly visible to all, rather than be secretly held onto by the one conducting the research.

However, the problems posed by `<choice>` elements are not the final impediment to creating a version of the text suitable for analysis. The problems which scribal errors that are not contained within a `<choice>` element and orthographic variants pose for data analysis are more complex. While there will be an explanation of how these two factors are dealt with, for now an overview of what is meant by scribal errors and orthographic variants is required. The difference between these two is subtle, but important. Both refer to the misrepresentation of a word with respect to modern standards of Greek orthography. Put simply, a scribal error refers to the incorrect spelling or incorrect placement of a diacritical mark. Ancient Greek orthography uses a number of different diacritical marks, like accents, breathing marks, and diaireses, which function along with letters to form the identity of a word. Thus, the incorrect placement of a diacritical mark can change the identity of a word entirely. An orthographic variant refers to the strict absence of an expected diacrtical mark. Ultimately, scribal errors are words rendered incorrectly. For example, a modern reader would expect the ancient Greek word for "wrath" to be written as μῆνις. If the scholiast were to add an additional letter to the word (μῆννις) or if he were to change the accentuation (μὴνις), these mutations would alter the very identity of the word and thus be considered scribal errors. Conversely, orthographic variants are equally valid forms of words, but do not follow modern orthographic standards. The Venetus A was written over a thousand years ago in the Byzantine era, so while many of the conventions of ancient Greek orthography are similar, many are indeed different. Returning to the example of "μῆνις," if the scholiast were to leave out the accent mark from an otherwise correctly spelled word (μηνις), the lexical identity remains intact. Thus, for our purposes, "μῆνις" is orthographically equivalent to the unaccented "μηνις". 

It should be clear that the presence of these Byzantine alternative forms pose difficulties for textual analysis. If one wanted to know how many times the nominative form of the word "μῆνις" appears in the scholia, one would have to keep in mind that the word could appear both as μῆνις and as μηνις. Instead it would be much preferrable if there were some way to convert all of the Byzantine forms to their modern orthographic equivalents. Unlike abbreviations discussed above, the HMT does not include any special mark-up to highlight the presence of a Byzantine equivalent; μηνις simply is transcribed as "μηνις". However, as part of their editing process, the HMT does try to validate their work to ensure it is as accurate as possible. Part of this validation process is ensuring that each word of the HMT text is a valid Greek word, and it does this by running each word through a Greek parser. The automated validator will then collect all of the words which failed to parse, and it becomes the editors' task to ensure that these words which do not meet modern standards of orthography were transcribed accurately. If a word is transcribed accurately but fails to parse because it is a Byzantine alternative form, the editors are then responsible for adding this word to the collections of words with verified Byzantine alternates, and the editors must also supply how the word would be written according to modern orthographic conventions. At the time of writing, this comprehensive list includes over 3000 examples of Byzantine orthographic variants.

What had to be done then, as in the case of those words with a `<choice>` element, was the creation of a version of the scholia in which all the Byzantine variants were converted into their modern equivalents. This was simply accomplished by writing a script which compared every word of the scholia to the HMT collection of Byzantine equivalents, and if the word indeed was a Byzantine equivalent, it was replaced with its modern equivalent. In order to create this text, the previously created normalized version of the text was used, rather than the text straight from the HMT archive. 

When this process was being carried out, there was some concern that some of the Byzantine versions of the words might have two or more modern equivalents, making a one-to-one conversion from Byzantine standards to modern standards impossible. For example, if there is just a plain letter eta, η, with no accent or breathing mark, this could technically be a number of things. It could be ἡ the feminine definite article, ἦ the interrogative particle, ἤ the conjunction, and so on. Thus when the modernized version of the text was first being created, we wrote the script such that if a Byzantine form matched with multiple modern equivalents, the script would convert the Byzantine form into a concatentation of all the modernized equivalents, separated by an underscore. So the plain, letter η  would be concerted to something like ἡ_ἦ_ἤ_. Once this edition was created, all of these concatenated words were searched for and analyzed to see if they truly represented different words. There were seventy-five instances where a Byzantine form had multiple modernized equivalents according to the HMT dataset, but virtually none of these seventy-five occurrences represented a case where there would be actual ambiguity. Most often, the presence of two modernized equivalents stemmed from their being one equivalent whose final syllable had a grave accent, and the other equivalent whose final syllable ending had an acute accent. However, the difference between a grave and acute accent on the final syllable is fairly inconsequential to the identity of a word since the presence of one or the other is entirely dependent on the presence of an enclitic or a subsequent word. Thus, for all intents and purposes, these differences between a final grave accent and a final acute accent are rendered meaningless, and the potential ambiguity of the Byzantine orthography is cleared up. So, despite their being seventy-five instances where a Byzantine orthographic variant could be matched to multpile modernized forms, only one presented a case of genuine ambiguity. Thus, the script which produces the modernized version of the scholia was modified so that if a Byzantine variant could be matched to multiple modernized forms, it would arbitrarily match with the first one, since virtually all the modernized forms were shown to be identical. 

The one case where there was genuine ambiguity ended up being the plain letter η, described above. All the possible modernized matches for η are fairly common words and are translated as "who", "the", "or", etc. Such words do not add much to the meaning to a Greek sentence, just as their English equivalents do not add much meaning to an English sentence. Thus it was decided that it was better to allow η to be substituted abritarily to its first modernized match. The goal of this modernized version of the text is not necessarily to be more readable, but rather to aid in text analysis and often in textual analyses the most common words are disregarded. Thus, there were no qualms about converting the plain letter η to its first modernized match.

It should be noted that the modernized version of the text was created using the previously created normalized text described earlier, and not using the text straight from the HMT archive. Since the normalized text already did some work towards creating a parseable text by disambiguating `<choice>` elements, it made sense to use this version of the scholia when trying to create a text that is even more ready for textual analysis. Just as with the normalized text and the diplomatic text, this modernized text represents a new version of the scholia, and thus a distinct URN citation was granted to these modernized scholia, which represent the third version of the scholia created so far.

While now all `<choice>` elements have been reduced to a single lexical token, and Byzantine orthographic variants have been updated to meet modern standards, the text is not yet ready for many of the analyses required by this thesis. This time, it is the inflected nature of the Greek language which poses even more difficulties for textual analysis. As stated previously, the forms of Greek words change depending on their function within a sentence. Thus, the iota-sigma ending of "μῆνις" indicates that it is most likely the subject of a sentence or clause. "μῆνιν" still means wrath, however the iota-nu ending indicates that it is most likely the direct object of some verb. The inflection of verbs is even more complex than that of nouns and adjectives. A verb can have hundreds of different forms based on factors such as person, i.e. whether I, you, or he performs an action, or tense, i.e. whether an action is happening in the present, past, or future. Because of the various complexities that are inherent to Greek corpora, it is necessary to normalize the forms of Greek words so that all of the variants are accounted for. This need to normalize is not unique to the Greek language. When working with an English corpus, one might want "am," "is,"  "was," etc., to be identified as forms of the verb "to be." Thus, with respect to this Greek corpus, no matter the inflection of "μῆνις," it should always be identified as the same lexical entity. 

For English corpora, the process of normalization is relatively simple as most English words do not vary widely in their morphology. In the case of nouns, for example, pluralization is their only possible morphological change, and this change affects only their endings. The root, or stem, of a noun usually does not differ between singular and plural forms. Even in the slightly irregular case of "child/children," though the pluralization is not simply the addition of an "s," the stem "child" is unaltered. There are, of course, exceptions to this rule of fixed stems (mouse/mice, goose/geese), but largely this is a constant feature of English words. Even among more mutable parts of speech like verbs, stems largely remain constant in English morphology. As such, there are many programs which "stem" English texts automatically (snowball stemmer is one such program). The result is a version of the original text in which the morphological variants of each individual word are replaced with a single normalized form. For example, a simple stemmer would read in the sentence, “The men carried, the women carry, the boy carries, and the girl is carrying,” and replace each form of “carry” with a single, truncated form like “carri.” 

Due to the greater complexity of Greek morphology, such simple stemming processes are inappropriate for managing Greek corpora. The only factors which can change the stem of an English verb are tense and voice (i.e. whether the verb is active or passive). Active verbs are those whose subjects perform the action, whereas passive verbs are those whose subjects are acted upon. For example, the boy actively carries, while the man is passively carried. Tense and voice are the same factors which can change the stem of a Greek verb. Where the two differ, however, lies the number of principal parts and the number of possible inflected forms of those principal parts in each language. Verbs in English have at most three distinct principal parts and they often share the same stem across all three (e.g. carry, carried, carried). In Greek, most verbs have six principal parts, none of which are guaranteed to share the same stem. For example, “φέρω, οἴσω, ἤνεγκον, ἐνήνοκα, ἐνήνεγμαι, ἠνέχθην” are the principal parts of the verb "to carry." Furthermore the number of principal parts is not the only complicating factor for Greek verb morphology. In English, even an irregular verb such as “to go” has only five possible morphological variants: go, goes, going, went, and gone. Greek verbs, on the other hand, have far more than five possible forms, since each principal part supplies the stem for a myriad of forms that change based on person, number, and mood of the verb. Due to the wealth of diversity within Greek morphology, stemming is not an effective method for normalizing Greek corpora.

These problems can in part be solved by the more advanced process of parsing. This is because, unlike stemmers, a competent parser is able to recognize the lemma, or the dictionary form, of a word and give a detailed report of its exact form, i.e., in the case of a noun, its person, case, and number. Note, this use of "lemma" referring to a word's entry in a dictionary is different from the use of "lemma" to describe the words which identify to what part of the Iliad a scholion a refers. As opposed to a stem, which is the root of a word to which endings are added, a lemma is the dictionary form of a word. "Carries" would have a stem of "carri" but a lemma of "carry." Morpheus, a parsing tool created by the Perseus Project, satisfies the requirements for a competent Greek parser. To review, the ultimate purpose of using a parser at this stage in this research is the creation of a text that simplifies the morphological variants of words into single, normalized forms. Thus, using the Morpheus parser allows for the conversion of inflected Greek forms into their respective lemmata. By incorporating Morpheus into a scala script, a text suitabe for textual analysis is created. The created text simply replaces the words of the orignial text with their respective lemmata. Thus, no matter whether the original text had "μῆνιν" or "μῆνις," a completely parsed version would not differentiate between the two; both would be recorded as "μῆνις". 

Ultimately, the Morpheus parser is able to reduce a large number of the words within the scholia, though there are some words that simply cannot be parsed. Scribal errors, such as misspellings and misplacement of an accent mark, which are not corrected by the scribe himself have no way of being accurrately parsed. These will simply appear within the parsed version of the scholia as they appear on the page of the manuscript. The hope is, however, that the number of blatant scribal errors is far fewer than the number of instances where a word can be successfully parse. In fact, while there are 149,202 total words in this dataset of Venetus A scholia, there only 35,710 unique words within the scholia. Since this parsed version is being created using the version of the text devoid of Byzantine orthographic variants, those 35,710 unique words do not include orthographic variants. Still, despite the high number of unique words in the Venetus A scholia, 27,116 (75.9%) of the unique words could be successfully parsed by the Morpheus parser. While the fact that 25% of the unqiue forms within the Venetus A cannot be parsed is a bit discouraging, it is a fact of digital humanities that perfect data is nearly impossible to come by.

When the modernized version of the text was created, it was important to consider whether there would be any ambiguity if a Byzantine orthographic variant could be represented by two or more modern equivalents. While this ambiguity ended up being negligent, this same consideration of possible ambiguity was taken up when creating this parsed version of the text. To understand this in terms of the English language, the word "lay" could be parsed in multiple ways. Either it is the present form of the verb "lay" or the past form of the verb "lie." This sort of lexical ambiguity happens far more often in Greek, however. To give this assertion some context, of the 27,116 words which could be successfully parsed by Morpheus, 4,909 of these words (18.1%) could be attributed to more than one lemma. These roughly 4,909 instances of ambiguity are much harder to simply ignore than the 75 instances of ambiguity in the Byzantine orthography list. Additionally, the cases of ambiguity caused by multiple lemmata cannot be simply attributed to a difference in accentuation on the final syllable, as was the case for the Byzantine orthographic lists. For many of the words which have multiple lemmata, the lemmata are often share the same root meaning but are different parts of speech. For example, βλάψει could come from either the noun βλάψις, meaning "harm," or the verb βλάπτω, meaning "to harm." In either case it is clear that βλάψει has something to do with harm, but I myself have no way of knowing whether the scholiast intended to use the verb or the noun. Were there only a few examples of ambiguity, it would be a worthwhile process to individually disambiguate the different possibilities. However, the disambiguation of all 4,909 words would be a thesis in itself. Since, the goal of a parsed edition is to reduce as much as possible the variant forms of a word to a single word, this edition is still fairly successful, even if not perfect. For example, if the form βλάψεις were to appear in the scholia, it too would share the same double lemmata, βλάψις_βλάπτω_, as the form βλάψει. Thus the two forms,βλάψεις and βλάψει, are still reduced to a single lemma, even if this lemma is imperfect. A better example rests in the case of the two forms ἥ and ὅ which, to the uninitiated eye appear to the unrelated to each other. However, the former could be either the feminine relative pronound or the feminine definite article. The latter could be, amogn other things, the masculine relative pronoun or the masculine definite article. As such the two are related and end up sharing the same double-lemmata, ὁ_ὅς_, the former lemma being the definite article and the latter being the relative pronoun. These two forms are exceedingly common words, so the ability to reduce them both to a single form make dealing with them much easier.

In sum, the final version of the parsed text is a much messier dataset than the version of the text that arose from the normalized or modernized version of the text. However, the goal of this parsed text is not to read it, but rather use it in text analysis. As such, many text analysis require the most common words to be excluded from analysis since they do not add much substance to the content of a text. It just so happens, however, that many of the words which have multiple lemmata are exceedingly common words which would be excluded from analysis anyways. So in the end, the parsed version is just another level of text wrangling that can lend itself to certain types of analyses. 

All four versions of the text whose creation was described in this chapter will be consulted in various forms throughout the rest of the thesis since different versions of the text have different advantages. If one wanted to analyze *exactly* what is written on a page, then one should consult the diplomatic edition. If one wanted to consult the most readable version of the scholia, then the modernized version should be consulted, and so on. Furthermore, many other versions of the text can be created in order to suit one's needs. In the following chapters, a fifth version of the text, one suited for topic modelling will also be discussed. What is important is that each of these new editions be appropriately cited so that they can both be related back to the original edition of the scholia sitting in the Homer Multitext archive, but also that the various editions can be distinguished from one another and be compared.

After this lengthy process of text wrangling, at last there exists a number of editions of the text which can be analyzed through macroanalysis. That distant-reading which Matthew Jockers describes at length in his book can finally be applied to the scholia. The next chapter will detail the specific methods within macroanalysis that were employed to elucidate hidden patterns of language within the scholia. The point to take away from this chapter is that digital corpora are often very messy and it is difficult to prepare them so that they can be sufficiently analyzed. At least for this thesis, an entire semester was spent writing and debugging scala scripts in order to create a usable text for analysis. Even then, there is still more work to be done in order to make the data as clean as possible. For having clean data is one of the most important factor in ensuring that one's results hold any sort of significance. So although it may seem like a hassle, this data preparation is the most important step in any digital humanities work.
