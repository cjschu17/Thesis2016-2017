##Theory and Methodology

- Intro to distant reading/ TM? (Jockers)
- How the data is originally structured (aka XML)
- Trying to take a text in xml format and analyze it (text only)
- In order to do this, must extract text
- Can't work with raw text (inflected language, BYZORTHO)
- For English, use stemmer; for Greek, use parser
- methods
- Tie Back to the beginning with topic modelling / word vector explanation



--------------------





---------------------



For any research such as this, where textual analysis is heavily reliant upon digital technologies, the necessary first step is to make the text of interest machine-readable. For example, a concatenation of every non-white space character in the first line of Poe's "The Raven" can be read by a human, albeit with some difficulty. On the other hand, this string of characters, "`Onceuponamidnightdreary,whileIpondered,weakandweary,`", is read by a computer as just that: a seemingly random collection of characters. Relying on digital techonologies to identify the meaningful word breaks in this example would certainly prove difficult, rendering the above string essentially useless for any textual analysis. While it seems unlikely that one might have such a concatenated text, when texts are being culled from various webpages and outside sources for analysis, the transformation of the text from its original source can often result in an unexpectedly poor and useless format. Thus importing an already-fashioned text requires a fair amount of careful prep work, or text wrangling, in order for it to suit one's analysis.


Luckily, in the case of this work on Iliadic scholia, the text of the scholia has already been written in a format that lends itself easily to digital analysis. This is because, as mentioned previously in chapter 1, the Homer Multitext project has been working for the last nine years to create digital editions of entire Iliad manuscripts. The scholia to the Venetus A manuscript have been transcribed in an XML format with separate editions for each book and type of scholia. By writing in XML, or Extensible Markup Language, the editors of the Homer Multitext project are able not only to record the visible letters on the manuscript page, but also to apply "markup" in order to supply more information about the text. Specifically, the Homer Multitext project follows the guidelines of the Text Encoding Initiative (TEI), a community of digital humanists who have established standards for editing texts. These guidelines inform the members of the Homer Multitext project on how to create digital editions which include additional information that is easily understood by other scholars. This extra information can convey how the scholia are ordered on the page, to which line of the poem a scholion refers, and whether a string of text in the scholion is a quotation from elsewhere in the *Iliad*. These and other more specific TEI elements will be explained as they become relevant to the research of the scholia. However, this section will focus on the structural aspects of the TEI guidelines as they relate to text wrangling.


The Homer Multitext project uses the same basic structural format for editing every scholion. That is, within one TEI element `<div>` there are three parallel subdivisions which each carry information about the scholion. The first subdivision contains just the lemma of the scholion, the second contains a uniform citation for the line of the *Iliad* on which the scholion is commenting, and the final subdivision contains the actual textual content of the scholion itself. Such a logical structure is essential for the creation of a digital scholarly edition. However, this additional structural information hinders textual analysis. While something like `<div type="text"><l>Wrath, sing, oh goddess</l></div>` is a valid and logical XML transcription of the firt few words of the *Iliad*, clearly text like `<div>` and `<l>` should not be included in an analysis. If one were working with a small amount of scholia, it may make sense just to manually extract all the text, excluding the lemmata and citations. The 8,000 scholia of this dataset, however, renders manual extraction impractical. In order to avoid this labor-intensive process, we wrote a program using scala, a computer programming language, which was able to perform this extraction automatically.


Even with a complete collection of words from all the scholia, the dataset would still be insufficient for any serious text analysis if left unaltered. The words in this collection have been transcribed exactly as they appear on the manuscript page. This means that any  orthographic variants, scribal errors, and even leftover markup from the editing process can clutter up the collection. In regard to the leftover markup, it can easily be extracted from the text of the scholia. The markup is written in English and the scholia are written in Greek; thus one can simply filter out the letters of the English alphabet from the surrounding Greek. 

The problems which scribal errors and orthographic variants pose for data analysis are more complex and will receive more attention in the later discussion concerning the parsing of a text. For now, just an overview of what is meant by scribal errors and orthographic variants is required. The difference between these two is subtle, but important. Both refer to the misrepresentation of a word with respect to modern standards of Greek orthography. Put simply, a scribal error refers to the incorrect spelling or incorrect placement of a diacritical mark. An orthographic variant refers to the strict absence of an expected diacrtical mark. Ultimately scribal errors are words rendered incorrectly. For example, a modern reader would expect the ancient Greek word for "wrath" to be written as μῆνις. If the scholiast were to add an additional letter to the word (μῆννις) or if he were to change the accentuation (μὴνις), these mutations would alter the very identity of the word and thus be considered scribal errors. Conversely, orthographic variants are equally valid forms of words, but do not follow modern orthographic standards. Instead, they follow standards which were accepted at the time of writing. Returning to the example of "μῆνις," if the scholiast were to leave out the accent mark from an otherwise correctly spelled word (μηνις), the lexical identity remains intact. Thus, for our purposes, "μῆνις" is orthographically equivalent to the unaccented "μηνις". How exactly such noise is handled within this corpus of Greek scholia will be discussed later, but it should be clear that the presence of alternative forms of the same word poses difficulties for textual analysis.

The inflected nature of the Greek language poses even more such difficulties for analysis. The forms of Greek words change depending on their function within a sentence. Thus, the iota-sigma ending of "μῆνις" indicates that it is most likely the subject of a sentence or clause. "μῆνιν" still means wrath, however the iota-nu ending indicates that it is most likely the direct object of some verb. The inflection of verbs is even more complex than that of nouns and adjectives. A verb can have hundreds of different forms based on factors such as person, i.e. whether I, you, or he performs an action, or tense, i.e. whether an action is happening in the present, past, or future. Because of the various complexities that are inherent to this dataset, it is necessary to normalize the forms of words so that all of the variants are accounted for. This need to normalize is not unique to Greek corpora. When working with an English corpus, one would want "am," "is,"  "was," etc., to be identified as forms of the verb "to be." Thus, with respect to the Greek corpus, no matter the inflection or accentuation of "μῆνις," it should always be identified as the same lexical entity. 


For English corpora, the process of normalization is relatively simple as most English words do not vary widely in their morphology. In the case of nouns, for example, pluralization is their only possible morphological change, and this change affects only their endings. The root, or stem, of a noun usually does not differ between singular and plural forms. Even in the slightly irregular case of "child/children," though the pluralization is not simply the addition of an "s," the stem "child" is unaltered. There are, of course, exceptions to this rule of fixed stems (mouse/mice, goose/geese), but largely this is a constant feature of English words. Even among more mutable parts of speech like verbs, stems largely remain constant in English morphology. As such, there are many programs which "stem" English texts automatically (snowball stemmer is one such program). The result is a of the original text in which the morphological variants of each individual word are replaced with a single normalized form. For example, a simple stemmer would read in the sentence, “The men carried, the women carry, the boy carries, and the girl is carrying,” and replace each form of “carry” with a single, truncated form like “carri.” 

Due to the greater complexity of Greek morphology, such simple stemming processes are inappropriate for managing Greek corpora. The only factors which can change the stem of an English verb are tense and voice (i.e. whether the verb is active or passive). Active verbs are those whose subjects perform the action, whereas passive verbs are those whose subjects are acted upon. For example, the boy actively carries, while the man is passively carried. Tense and voice are the same factors which can change the stem of a Greek verb. Where the two differ, however, lies in the number of principal parts and the number of possible inflected forms of those parts. Verbs in English have at most three distinct principal parts and often share the same stem across all three (e.g. carry, carried, carried). In Greek, most verbs have six principal parts, none of which are guaranteed to share the same stem. For example, “φέρω, οἴσω, ἤνεγκον, ἐνήνοκα, ἐνήνεγμαι, ἠνέχθην” are the principal parts of the verb "to carry." Furthermore the number of principal parts is not the only complicating factor for Greek verb morphology. In English, even an irregular verb such as “to go” has only five possible morphological variants: go, goes, going, went, and gone. Greek verbs, on the other hand, have far more than five possible forms, as each principal part supplies the stem for myriad forms that change based on person, number, and mood of the verb. Due to the wealth of diversity within Greek morphology, stemming is not an effective method for normalizing Greek corpora.

The problems can in part be solved by the more advanced process of parsing. This is because, unlike stemmers, a competent parser is able to recognize the lemma of a word and give a detailed report of its exact form, i.e., in the case of a noun, its person, case, and number. As opposed to a stem, which is the root of a word to which endings are added, a lemma is the dictionary form of a word. "Carries" would have a stem of "carri" but a lemma of "carry." Morpheus, a parsing tool created by the Perseus Project, satisfies the requirements for a competent Greek parser. To review, the ultimate purpose of using a parser at this stage in this research is the creation of a text that simplifies the morphological variants of words into single, normalized forms. Thus, using Morpheus allows for the conversion of inflected Greek forms into their respective lemmata. By incorporating Morpheus into a scala script, a text suitabe for textual analysis is created. The created text simply replaced the words of the orignial text with their respective lemmata. Thus, no matter whether the original text had "μῆνιν" or "μῆνις," a completely parsed version would not differentiate between the two; both would be recorded as "μῆνις". 

If every word of the original of the text were able to be successfully parsed by Morpheus, there would be no need for further discussion about how to fashion a text for textual analysis. However, the previously discussed orthographic variants pose significant problems for analysis. To recap, an orthograhpic variant is a form of a word which is spelled correctly, but missing a breathing mark, an accent, or some other diacritical mark. These variants often arise from the fact that the manuscript was written in the Byzantine period, when the conventions of writing Greek may have been slightly different than in modern printed editions. So μῆνις and μηνις are considered Byzantine orthographic equivalents. These orthographic variants occur very frequently in the edition of the scholia being used due to the editing process of the Homer Multitext project. For its goal is to faithfully recreate the exact markings on each page such that even the most obvious spelling or orthographic differences are recorded as they appear. Thus, the very common preposition ἀπό, meaning "away" or "off", is recorded many times in many different forms: with its accent, without its accent, with its breathing mark, etc. As a result, these variants are so numerous that they cannot be ignored. 

Currently, though, there is no way to effectively account for these orthographic variants in our analysis. There is a comprehensive collection of every Byzantine orthographic variant that the Homer Multitext project has encountered, but this has not yet been incorporated into this analysis. Nor does it seem like given the complexity of incorporating the list that it will be a part of this thesis. Hopefully, future work from the Homer Multitext project will make this a possibilty, but in the interest of time it was deemed better to have imperfect data rather than no data. 

Fortunately, the process of running a topic model, for instance, includes a step where one can filter out specific words from the dataset. Most often these so-called stopwords are the most frequently occurring words in a corpus. In the case of the scholia, if a certain Byzantine orthographic variant shows up so often that it begins to confound the data, one can simply add that variant to the list of stop words. While it is feasible to add every single Byzantine orthographic equivalent to the list of stopwords, ultimately this led to less valuable results. So although the data might be "cleaner" without the orthographic variants, these seem to be important for a successful topic model. Topic modelling relies on analyzing the collocation of frequently occuring words and some frequently occurring words appear almost always as an orthographic variant. So if all of the variants are stripped out completely, then the text begins to miss many of these significant words and the resulting topic model is weaker. 

So while an edition of the text where all of the Byzantine orthographic equivalents are replaced with their normalized forms would be ideal, such an edition is actually not necessary for topic modelling. Current topic models run succesfully using the text which contains the orthographic variants. Thus, the normalization of the orthographic variants is not a mandatory prerequisite for this project.

This lengthy process of text wrangling at last yields an edition of the text which is able to be analyzed through distant reading, a discussion of which began this chapter. 

