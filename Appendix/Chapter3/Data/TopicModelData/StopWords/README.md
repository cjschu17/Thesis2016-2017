The files here represent data necessary for the creation of the list of stop words need in order to get accurate data from the topic modelling software. Since the "topics" or a topic model are nothing more than repeating patterns of co-occurring words, if a frequently-occurring words, like the definite article, occurs so frequently that the topic modelling software will perceive patterns of co-occurence, i.e. topics, that do not make sense. Thus, their removal from the dataset is absolutely important.

Here, one can see the list of every word in the Venetus A along with how frequently that word occurs. This data is available in the file [MostFrequentWords.tsv](https://github.com/cjschu17/Thesis2016-2017/blob/master/Appendix/Chapter3/Data/TopicModelData/StopWords/MostFrequentWords.tsv). This list is based on the morphologically-normalized (m-normalized) version of the scholia fround [here](https://github.com/cjschu17/Thesis2016-2017/blob/master/Appendix/VersionsOfScholia/m-normalized.tsv).

When creating which words should be eliminated from the dataset, I only looked at the 250 most frequently-occurring words. The final list of words eliminated from my dataset in order to create the version of the text suitable for topic modelling is found in the fie [stopWords.txt](https://github.com/cjschu17/Thesis2016-2017/blob/master/Appendix/Chapter3/Data/TopicModelData/StopWords/stopWords.txt).
