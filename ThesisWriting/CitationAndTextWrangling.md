Even if the content of this thesis seems to focus strongly on the analysis of computer-generated data, the question which provides the basis for this thesis is one which has perplexed scholars of the Venetus A for centuries. As explained in the previous chapter, there has been no consensus on how to interpret the unique five-zoned layout of the Venetus A scholia. As far back as 1875, the prominent Homeric scholar Karl Dindorf made a number of significant conclusions about different aspects of the scholia, but according to modern standards, his methodology undermined the strength of many of his conclusions. In his whole discussion of the nature of intermarginal, interior, and main scholia, he only mentions a handful of examples. In essence, he requires his readers to rely just on his ethos as a scholar to support his final conclusions. This is not to say that Dindorf's conclusions are incorrect, nor is it fair to say that Dindorf was an inferior scholar conducting poor scholarship. It may be exceedingly possible that Dindorf had a wealth of examples to support his conclusion and a print format prevented Dindorf from filling his introduction with too many examples.

However, even if Dindorf had included every single piece of evidence he had collected to support his conclusions, his methodology would still put his findings under suspicion. Dindorf and nearly every other scholar prior to the rise of digital technologies relied on close-reading for any and all textual analyses. As Matthew Jockers notes in his book on the method of "distant-reading," close-reading is best applied either to individual texts or to a small subset of them (19). Thus, something like a single poem lends itself well to a close-reading, but the entire corpus of British novels does not. It follows, then, that the estimated ten thousand scholia of the Venetus A manuscript constitute too large of a corpus for close-reading to be effective. Considering also that the scholia are a diverse set of documents, it would be unlikely that Dindorf or any other scholar could make many intelligible comments about minute linguistic patterns over so large and varied a corpus. Therefore, criticizing Dindorf or most of the previous Venetus A scholars is not quite fair, since they did not have the tools to analyze the text in a more systematic fashion.

Simply put, a computer is required for the "distant-reading" which the Venetus A scholia demands. Jockers defines this "distant-reading" as a sort of macroanalysis that, while devoid of "human synthesis and intuition," can process much more information and in much less time (19). Jockers' book details many examples of valid scholarly questions about literature that simply cannot be answered using close-reading alone. Typically, his examples deal with massive amounts of data, such as the entire corpus of nineteenth-century Irish literature. However, Jockers is by no means advocating for the abolition of close-reading in favor of his "distant-reading." Instead, he compares the relationship between close and distant reading to that of micro and macroeconomics. He writes, "Just as microeconomics offers important perspectives on the economy, so too does close-reading offer fundamentally important insights about literature" (26). Ultimately, he concludes that mining data from macroanalysis is also not enough, but rather "human interpretation of the data... remains essential" (26). Thus, distant-reading is meant to complement close-reading and not replace it.

The number of avenues for pursuing such distant-reading continues to grow as the field of digital humanities itself continues to grow. A fuller description of the technologies applied in this thesis, such as topic modelling, will be put forth later when it becomes more relevant.

First, however, it is necessary to discuss the first step in any research where textual analysis is heavily reliant upon digital technologies. One must make the text or texts of interest machine-readable before any analysis can be carried out. For example, a concatenation of every non-white space character in the first line of Edgar Allen Poe's "The Raven" can be read by a human, albeit with some difficulty. On the other hand, this string of characters, "`Onceuponamidnightdreary,whileIpondered,weakandweary,`", is read by a computer as just that: a seemingly random collection of characters. Relying on digital technologies to identify the meaningful word breaks in this example would certainly prove difficult, rendering the above string essentially useless for any textual analysis. While it seems unlikely that one would ever encounter such a concatenated text, texts are often transformed in unexpected ways when one collects them for analysis from various webpages and outside sources. Scholars are not usually interested in analyzing texts which they themselves have created, but rather those which are available elsewhere, such as from an online archive. Thus importing an already-fashioned text requires a fair amount of careful prep work, or text wrangling, in order for it to suit one's analysis.

Still, even the line `Once upon a midnight dreary, while I pondered, weak and weary,` would not be ready for most forms of textual analysis. One the one hand, it is true that the above text could be more easily broken into individual words and analyzed by a computer. But while a human might recognize a list of individual words `Once` `upon` `a` `midnight` etc. as being the opening words of Poe's poem, a computer will not. To the computer it is just a list of seemingly random words. What is needed is some sort of metadata which is connected to the text that allows the computer to recognize that "Once upon a midnight dreary..." should not be considered equivalent to "Two roads diverged in a yellow wood." In other words, in order to create machine-readable texts, the text has to be cited.

The simplest way of creating a citable text of a work is to prepare a text in two columns. The first column carries a citation, the second column carries the text. The depth of one's citation depends heavily on what sort of analysis is required. It is reasonable that if one wanted to compare the entire corpus of Robert Frost poems, one would have the first column contain a citation of a particular poem and the second column contain the entire poem. More commonly though, scholars like to refer to chapters of a book, or lines of a poem. Figure 5 demonstrates what a citable text version of the first two lines of Frost's "The Road Not Taken" might look like:

!["The Road Not Taken"](./images/urnExample1.png)

 
While Figure 6 demonstrates what this would look like for the first two lines of "Stopping by Woods on a Snowy Evening":

!["Stopping by Woods on a Snowy Evening"](./images/urnExample2.png)

urn:cts:EnglishLit.American.Poetry:Robfrost1.poetryFound:1

While this Robert Frost citation scheme is merely hypothetical and not actually in use anywhere, my example is based upon an actual model for creating citable texts: the Ordered Hierarchy of Citation Objects (OHCO2). The above example demonstrates the prominent features of the OHCO2 model which are fully explained in an article by its creators Neel Smith and Gabriel Weaver. The OHCO2 model defines a text as "a set of citable nodes," which in this case would be a line of poetry ("OHCO2 Model"). Each line of the poem is situated within a larger citation scheme that allows it to be distinguished from a line of poetry in some other work. As can be gleaned from the above example, the two citations of the two Frost poems are nearly identical except for one being "RobFrost1" and the other being "RobFrost2". The designations of "RobFrost1" and "RobFrost2" are arbitrary, but they have to be consistent. Within this schema, "RobFrost1" must be used only to refer to "The Road Not Taken". Furthermore, there was no need for me to use numbers for this labell - the two poems could have easily been cited using "RobFrostA" and "RobFrostB". One step further, the label need not make logical sense. `urn:cts:EnglishLit.American.Poetry:asdof` might be used to refer to "The Road Not Taken" while `urn:cts:EnglishLit.American.Poetry:oiunji` might be used to refer to "Stopping by Woods." As long as the citation scheme is unambiguous and consistent, there are no issues. 

Directly  before "RobFrost1" and "Robfrost2" comes a colon, and directly after "RobFrost1" and "Robfrost2" comes a period. Labels within the citation are separated either by a colon or by a period. Colons separate the discrete units of the citation, whereas periods separate give information about the hierarchy within each unit. All citations in the OHCO2 model contain the same types of units in the same order. So, for example, the first unit in every single OHCO2 citation will be the string `urn`. This string does not carry any specific information about the text, rather it is indicating that the citation is following the format of the Uniform Resource Name (urn) schema. The second unit of the citation will always be the string `cts`. Similar to the previous unit, the `cts` does not give any specific information about the text itself. Rather the string defines the type of communication a server can have with this citation, specifically, it can communicate with it in ways following the Canonical Text Services (CTS) protocol. The specifics of this protocol fall beyond the scope of this thesis.

The third and fourth units of the citation do actually give specific information about the text. As seen in the example citation concerning Robert Frost, fourth unit `RobFrost1.poetryFound`, gave specific information about what text we were dealing with. While the `RobFrost1` has already been discussed as distinguishing "The Road Not Taken" from `RobFrost2` as "Stopping By Woods," it is worth explaining the function of the period and the subsequent `poetryFound`. As mentioned, periods in this schema separate and describe the hierarchy of a particular unit of the citation. So if the fourth unit refers to the specific work being cited, the periods describe the work hierarchy of a text. In this case `poetryFound` provides information about which specific source provided the text being cited. In this case, I found the text for the two Frost poems on the website *Poetry Foundation*, thus `poetryFound` is meant to reflect my direct source. The distinction here is ultimately between what the developers of the OHCO2 model refer to as the "notational text" and a particular version of a text. Whereas the citation urn:cts:EnglishLit.American.Poetry:Robfrost1 refers to Frost's "The Road Not Taken" in the abstract, urn:cts:EnglishLit.American.Poetry:Robfrost1.poetryFound refers to a version of the text that exists on a particular website, as opposed to one found in a print anthology.

Whereas the fourth unit of the citation refersto teh work hierarchy, the third unit refers to the bibliographic hierarchy.  Immediately to the left of the "RobFrost1" and "RobFrost2" is the string `EnglishLit.American.Poetry`. This label is providing information about the classification of Robert Frost's poem as first poems, within the more general classification of American literature, within the more general classification of literature written in the English langauge. With this understanding, the truncated citation `urn:cts:English.Amercan` would be an appropriate way to cite all of American literature. Again, all thesee labels are is arbitray. It could have been labeled `EngLit.USA.poems`, or even just a string of numbers like `123.534.2`. As long as the labels are unique and used consistently, any will work just fine. The specificity of the label just helps humans understand the purpose citation. A computer would not care if the label were "AmPoetry" or "12345". 

The fifth and final unit to be discussed is the text hierarchy which comes after "poetryFound". This is the unit of citation  which differentiates individual lines or chapters from one another. Simply stated, most books and poems have some sort of hierarchy which dictate in what order the lines or sentences are to be read. Thus there is a first line, followed by the second, followed by the third, and so on. This form of citation should be familiar to anyone who has read Shakespeare or most Classical poetry, where each line is given a unique identifier. No matter what version of the text one holds, there will always exist a line1 and line2 and line3 and so on. However, it is worth nothing that, again, the designations of "line1", "line2", and "line3" are ultimately arbitrary. Very often Classical scholars disagree about the order of lines in a text. However, the labelling of the lines of a poem in the field of Classics is version independent. So, for example, Figure 7 demonstrates a citable text version of the Loeb editions's first three lines of Aeschylus's *Agamemnon*:

![Loeb *Agamemnon*](./images/ag1.png)

Figure 8, however, shows how the citations would change if I for some reason thought that the order should be different:

![Edited *Agamemnon*](./images/ag2.png)

One should notice that the same numbered labels apply to the same lines. Line "1" still identifies the line beginning "θεοὺς μὲν," even though line "1" is the second line of my edited version. This is because "1" and "2" are identifiers for the line, and not a description of where they appear in the passage. Their identification was originally based on the positioning of the lines in some version of the text, but they do not define the order for every version. Similarly, figure 9 presents a citable text version of the *Agamemnon* were I to delete line "2":

![Second Edit of *Agamemnon*](./images/ag3.png)

Now the line beginning "στέγαις Ἀτρειδῶν" is the second line of the play, but it still retains the identifier "3". In sum, the OHCO2 models allows for a disambiguation of lines according to a text's own internal hierarchy.

To recap, the OHCO2 citation model requires a citation which situates a particular text within the broader spectrum of the bibliographic hierarchy, similar to how the Dewey Decimal system works. Additionally, the citation must provide information about which particular version of a work one is dealing with, whether a specific print edition or just the more abstract notional text. Finally, it must give some information of the internal structure of the work itself, whether that means book chapters, paragraphs, sentences, lines of poetry, etc. 

Citing texts in such a systematic and orderly fashion is extremely useful for a number of reasons, but it is particularly useful as it allows scholars to have a pre-designed method for referring to passages of a text unambiguously. Different editions of a book may have different page numbers, but we presume that it will have the same or similar chapters and paragraphs. In other words the citation scheme is not dependent on any outside device like a page number. urn:cts:EnglishLit.American.Poetry:Robfrost1:1 would represent the first line of "The Road Not Taken" regardless if one person is reading the text on a computer screen and another in a print anthology. Furthermore, the citation can be understood by a computer as well, given that a program is written to follow the CTS protocol.

However, to reiterate, all the example citations given so far have been just that, examples. They work well in this context of explaining the importance of citing a text, but a computer without any prior programming would be just as confused as an English professor if it were expected to know that urn:cts:EnglishLit.American.Poetry:Robfrost1 was a citation for "The Road Not Taken." In other words, in order to have a meaningful citation, there needs to be an agreed upon classification of texts, like the Dewey Decimal system. Luckily, for the field of Classical studies, such an ordered index of all ancient Greek texts has already been put together by the Thesaurus Linguae Graecae, a library for ancient Greek literature. Thus, a citation like urn:cts:greekLit:tlg0012.tlg001 actually means something beyond an individual project. In this citation `urn:cts:` is still just functioning to define what type of citation scheme is being followed. The unit `greekLit` is making reference to a specific index of works, in this case the index of Greek works put together by the Center for Hellenic Studies (http://homermultitext.github.io/hmt-docs/faq/). The unit `tlg0012.tlg001` is the aspect that makes use of the index previously put together by the Thesaurus Linguae Graecae (tlg). `tlg0012` is the index number for Homeric poetry, and with the addition of `tlg001`, `tlg0012.tlg001` specifically refers to the *Iliad*. Again, CTS urn citations do not have to connect to outside indices and can simply be project specific. As long as the conventions of the citation are employed consistently and do not lead to ambiguous citations, the citations are valid. However, it can often be easier just to use indices created by an outside entitiy for the sake of simplicity.

If one wants to impose a rather strict citation scheme upon a text, it is best to fit the citation scheme to the apparent structure of the text. So, for example, if a text you want to cite a text like a poem which is broken out into lines, it would be best to impose a citation scheme that differentiates individual lines of poetry. In the case of this work on Iliadic scholia, the text of the scholia has already been written in a format that easily lends itself to citation as well as to digital analysis. This is because, as mentioned previously in chapter 1, the Homer Multitext project has been working for the last nine years to create digital editions of entire *Iliad* manuscripts. The scholia to the Venetus A manuscript have been transcribed by members of the project using an XML format. Each book of scholia, along with each type of scholia, is transcribed into its own separate document. By writing in XML, or eXtensible Markup Language, the editors of the Homer Multitext project are able not only to record the visible letters on the manuscript page, but also to apply "markup" in order to supply more information about the text. Specifically, the Homer Multitext project follows the guidelines of the Text Encoding Initiative (TEI), a community of digital humanists who have established standards for editing texts. These guidelines inform the project's policies on creating digital editions such that the markup can be understood easily by other scholars. This extra information can convey how the scholia are ordered on the page, to which line of the poem a scholion refers, and whether a string of text in the scholion is a quotation from elsewhere in the *Iliad*. These and other more specific TEI elements will be explained as they become relevant. However, this section will focus on the structural aspects of the TEI guidelines as they relate to text wrangling. 

For example, above it was said that it is important that a poem broken up into lines be cited as a set of lines. In XML, there is an element `<l>` which means "line," so one could just transcribe a poem into an XML format where each line is contained within its own `<l>` element. Additionally, one can add attributes to the element in order to add more information about the element. So for the element `<l>`, one of the attributes is `@n`, meaning "name". This `@n` attribute can be used to give some sort of identifier to a line. So logically, the first line of a poem might have the identifier `1`. So in total the first line of "The Road Not Taken" in XML might look something like: `<l n="1">Two roads diverged in a yellow wood.<\l>`

When it comes to creating an XML edition of the scholia, the Homer Multitext project uses the same basic structural format for every scholion. That is, within one TEI element `<div>`, "division", there are three parallel subdivisions, still using the element `<div>`, which each carry information about the scholion. The first subdivision contains just the lemma of the scholion. The lemma is a quotation from part of the *Iliad* line which appears at the beginning of a scholion and serves to clue the reader into which line of the *Iliad* the scholion is going to comment on. The second subdivision contains a URN citation for the line of the *Iliad* on which the scholion is commenting, and the final subdivision contains the actual text content of the scholion itself. Such a logical structure is essential for the creation of a digital scholarly edition.

Figure 10 demonstrates how one scholion would be marked-up in XML. This scholion is commenting on *Iliad* 18, line 4.

![A scholion to *Iliad* 18.4](./images/exampleXML2.png)


START HERE!!!!!!!!!!

While the XML mark-up helps keep a text structured and organized, this additional structural information hinders textual analysis. While something like `<div type="text"><l>Wrath sing, oh goddess, of son of Peleus Achilles</l></div>` is a valid and logical XML transcription of the first line of the *Iliad*, clearly text like `<div>` and `<l>` should not be included if one wanted to analyze the number of characters in the first line of the poem. If one were working with a small amount of scholia, it may make sense just to manually copy and paste all of the actual text out of the XML mark-up. The 8,000 scholia of this dataset, however, render manual extraction impractical. In order to avoid this labor-intensive process, my collaborator, Melody Wauke, and I wrote a program using scala, a computer programming language, which was able to perform this extraction automatically. Computer programming scripts written using scala were the primary way I conducted most of the research of this thesis.

Even with a complete collection of characters from all the scholia without any XML markup, such a dataset would still be insufficient for any serious textual analysis if left unaltered. The Homer Multitext creates diplomatic editions of manuscripts, meaning that every intentional mark is transcribed exactly as it appears on the manuscript page. This means that even if a scribe makes a spelling error that would be easily to fix, the manuscript is transcribed in such a way that the error is retained. Thus, various orthographic variants and scribal errors cluttered up our data. Additionally, there are other features of the manuscript's language, like abbreviations, which also clutter up the data. With some experience reading the manuscript, these abbreviations are fairly easy to understand. The string of characters, Ἀρίσταρχ, surely must be an abbreviation for Ἀρίσταρχος (Aristarchus), the Alexandrian editor. The HMT does not want to ignore the fact that the name was originally abbreviated, which would invalidate its claim that it is creating a diplomatic edition. However, refusing to expand the abbreviation would make the project's editions difficult to use in text analysis. This is because Ἀρίσταρχ does not have to expand to Ἀρίσταρχος, which is the nominative form of Aristarchus' name. Greek is an inflected language, meaning that endings of words change depending on the words function within a sentence. So Ἀρίσταρχ could also expand into Ἀριστάρχου, Ἀριστάρχῳ, or Ἀρίσταρχον depending on how Aristarchus is being used in the sentence. Thus, just knowing that Ἀρίσταρχ appears in the text would not allow for a complete analysis of the sentence structure, since the ending of Ἀρίσταρχ would be unknown.

The HMT has reached a compromise on this issue; they record both the abbreviated and expanded form of word in their XML edition, by utilizing the TEI element `<choice>` which allows two versions of the same word to be recorded. So in the example of Ἀρίσταρχ, it would be marked-up like what is seen in figure 11: 

![Aristarchus: abbreviated and expanded](./images/exampleXML.png)

While the mark-up seen in Figure 11 allows for editors to make a compromise between presenting a purely diplomatic edition, and a normalized, more easily readable edition, it poses a challenge for textual analysis. As can clearly be seen, if one were to simply extract all the text from the XML mark-up seen in figure 11, the texts Ἀρίσταρχ and Ἀρίσταρχος would both be reproduced. This obviously poses a problem for analysis since now there are two words present when there should only be one. If one wanted to know how often the string of characters Ἀρίστ appeared in the scholia, the result would be double what really exists. 

For this reason, and for others which will be explained below, it became necessary to rethink the approach to analyzing the text. Rather than just extracting all the text at once from the XML, it became necessary to create different versions of the text according to different standards of reading the text. For example, if one wanted a truly diplomatic edition, one where none of the abbreviations are expanded, then a text should be created wherein the XML-extracting script ignores every `<expan>` element. Similarly, if someone were interested in a more normalized text where every abbreviation *is* expanded, the XML-extracting script should ignore every `<abbr>` element. This same logic applied to instances where a scribe originally wrote one thing, but later corrected it. There should be a version of the text which includes only these original errors, and then a separate edition which records only the corrections of the scribes. This is exactly what was done by my advisor Neel Smith. There are now two separate editions of the scholia, one which selects the more diplomatic option at every `<choice>` element, and one which selects the more normalized option at every `<choice>` element.

What is important to note that these two separate versions of the scholia is that both the diplomatic and normalized versions can be cited within the already existing OHCO2 citation scheme for the scholia. Recalling the above discussion, the diplomatic and normalized versions would simply fall underneath the general work hierarchy. Just as in the example URN urn:cts:EnglishLit.American.Poetry:Robfrost1.poetryFound, `poetryFound` referred to the fact that this version of "The Road Not Taken" was from the website PoetryFoundation, the URN for the scholia could simply add `diplomatic` or `pnormalized` in order to specify which version of the scholia is being worked with. So urn:cts:greekLit:tlg5026.msA.hmt.diplomatic:1.1 and urn:cts:greekLit:tlg5026.msA.hmt.pnormalized:1.1 would both constitute a valid citation for the same scholion. Just as before `urn:cts:` simply give information about the structure of the citation, and `greekLit:` refers a specific index of Greek works collected by the Center for Hellenic studies. `tlg5026` represents the index number given to Iliadic scholia by the Thesaurus Linguae Graecae, while `msA.hmt` specifies that within the vast range of Iliadic scholia, this citation is dealing only with the Venetus A (msA), and only a version prepared by the Homer Multitext (hmt). Finally, this particular scholion is the first scholion of the first book of the Iliad (1.1). The only thing that differs is in the work hierarchy with either "diplomatic" or "pnormalized" appearing. The "p" before "normlaized" simply means that the text is being normalized based on paleographical standards. By treating the two versions of the text as their own separate editions, any subsequent analyses on the text of the scholia is entirely separate from the transcription put forth by the Homer Multitext project. That is, any arguments about the text can be made with clear citations to the exact version of the text being used (diplomatic, p-normalized, etc.). If this were not the case, the only existing citation would be the standard citation for the Homer Multitext's edition. However, the literal string of the HMT edition was shown to impede logical text analysis, and had to be altered. These version citations allow these necessary alterations to be plainly visible to all, rather than be secretly held onto by the one conducting the research. It should be noted that the urn citations to disambiguate the various versions of the scholia were not officially published at the time of writing, but the logic of maintaining different versions of the scholia was extremely important to my research. 

However, the problems posed by `<choice>` elements are not the final impediment to creating a version of the text suitable for analysis. The problems which scribal errors that are not contained within a `<choice>` element and which orthographic variants pose for data analysis are more complex. While there will be an explanation of how these two factors are dealt with, for now an overview of what is meant by scribal errors and orthographic variants is required. The difference between these two is subtle, but important. Both refer to the misrepresentation of a word with respect to modern standards of Greek orthography. Put simply, a scribal error refers to the incorrect spelling of a word or the incorrect placement of a diacritical mark. Ancient Greek orthography uses a number of different diacritical marks, like accents, breathing marks, and diareses, which function along with letters to givea waord its identity. Thus, the incorrect placement of a diacritical mark can change the identity of a word entirely. Ultimately, scribal errors are words rendered incorrectly. An orthographic variant, by contrast, refers to the strict absence of an expected diacritical mark. For example, a modern reader would expect the ancient Greek word for "wrath" to be written as μῆνις. If the scholiast were to add an additional letter to the word (μῆννις) or if he were to change the accentuation (μὴνις), these mutations would alter the very identity of the word and thus would be considered scribal errors. Conversely, orthographic variants are equally valid forms of words which do not follow modern orthographic standards. The Venetus A was written over a thousand years ago in the Byzantine era, so while many of the conventions of ancient Greek orthography are similar, many are indeed different. Returning to the example of "μῆνις," if the scholiast were to leave out the accent mark from an otherwise correctly spelled word (μηνις), the lexical identity remains intact. Thus, for our purposes, "μῆνις" is orthographically equivalent to the unaccented "μηνις". 

It should be clear that the presence of these Byzantine alternative forms pose difficulties for textual analysis. If one wanted to know how many times the nominative form of the word "μῆνις" appears in the scholia, one would have to keep in mind that the word could appear both as μῆνις and as μηνις. Instead it would be much preferable if there were some way to convert all of the Byzantine forms to their modern orthographic equivalents. Unlike the problems posed by abbreviations discussed above, the HMT does not include any special mark-up to highlight the presence of a Byzantine equivalent; μηνις simply is transcribed as "μηνις". However, as part of our editing process, we at the HMT do try to validate our work to ensure it is as accurate as possible. Part of this validation process is ensuring that each word of the HMT text is a valid Greek word. It does this by running each word through a Greek parser. The automated validator will then collect all of the words which failed to parse. Then an editor must ensure that these words which do not meet modern standards of orthography were transcribed accurately. If a word is transcribed accurately but still fails to parse because it is most likely a Byzantine alternative form. The editors are then responsible for adding this word to a collections of words with verified Byzantine alternates, and the editors must also supply how the word would be written according to modern orthographic conventions. At the time of writing, this comprehensive list includes over 3000 examples of Byzantine orthographic variants.

What had to be done then, as in the case of those words with a `<choice>` element, was the creation of a version of the scholia in which all the Byzantine variants were converted into their modern equivalents. I was able to accomplish this, along with Melody Wauke, by writing a scala script which compared every word of the scholia to the HMT collection of Byzantine equivalents, and if the word indeed was a Byzantine equivalent, it was replaced with its modern equivalent. In order to create this text, the previously created p-normalized version of the text was used, rather than the text straight from the HMT archive. 

When this process was being carried out, there was some concern that some of the Byzantine versions of the words might have two or more modern equivalents, making a one-to-one conversion from Byzantine standards to modern standards impossible. For example, if there is just a plain letter eta, η, with no accent or breathing mark, this could technically be a number of things. It could be ἡ the feminine definite article, ἦ the interrogative particle, ἤ the conjunction, and so on. Thus when the modernized version of the text was first being created, we wrote the script such that if a Byzantine form was associated with multiple modern equivalents, the script would convert the Byzantine form into a concatenation of all the modernized equivalents, separated by an underscore. So the plain, letter η would be concerted to something like ἡ_ἦ_ἤ_. Once this edition was created, all of these concatenated words were searched for and analyzed to see if they truly represented different words. There were seventy-five instances where a Byzantine form had multiple modernized equivalents according to the HMT dataset, but virtually none of these seventy-five occurrences represented a case where there would be actual ambiguity. Most often, the presence of two modernized equivalents stemmed from their being one equivalent whose final syllable had a grave accent, and the other equivalent whose final syllable ending had an acute accent. However, the difference between a grave and acute accent on the final syllable is inconsequential to the identity of a word since the presence of one or the other is entirely dependent on the presence of an enclitic or a subsequent word. Thus, for all intents and purposes, these differences between a final grave accent and a final acute accent are meaningless, and the potential ambiguity of the Byzantine orthography was nil. So, despite there being seventy-five instances where a Byzantine orthographic variant could be matched to multiple modernized forms, only one presented a case of genuine ambiguity. In light of this discovery, the script which produces the o-normalized version of the scholia was modified so that if a Byzantine variant could be matched to multiple modernized forms, it would arbitrarily match with the first one, since virtually all the modernized forms were shown to be identical. The "o" in "O-normalized" refers to the fact that the text is being normalized compared to orthographic standards.

The one case where there was genuine ambiguity among the moderized forms ended up being the plain letter η which was described above. All the possible modernized forms of η are fairly common words and are translated as "who", "the", "or", etc. Such words do not add much to the meaning to a Greek sentence, just as their English equivalents do not add much meaning to an English sentence. Thus it was decided that it was better to allow η to be substituted arbitrarily to its first modernized match than to work out how to disambiguate each instance of a plain eta. The goal of this o-normalized version of the text is not necessarily for it to be more readable, but rather for it to aid in text analysis and often in textual analyses very most common words are disregarded. Thus, there were no qualms about converting the plain letter η to its first modernized match.

It should be noted that the o-normalized version of the text was created using the previously created p-normalized text described earlier, and not using the text straight from the HMT archive. Since the p-normalized text already did some work towards creating a parseable text by disambiguating `<choice>` elements, it made sense to use this version of the scholia when trying to create a text that is even more ready for textual analysis. Just as with the p-normalized text and the diplomatic text, this o-normalized text represents a new version of the scholia, and thus a distinct URN citation should be created to disambiguate it from other versions of the scholia, but this was not done at the time of writing.

While now all `<choice>` elements have been reduced to a single lexical token, and Byzantine orthographic variants have been updated to meet modern standards, the text is still not ready for many of the analyses required for this thesis. This time, it is the inflected nature of the Greek language which poses the greatest difficulties for textual analysis. As stated previously, the forms of Greek words change depending on their function within a sentence. Thus, the iota-sigma ending of "μῆνις" indicates that it is most likely the subject of a sentence or clause. "μῆνιν" still means wrath, however the iota-nu ending indicates that it is most likely the direct object of some verb. The inflection of verbs is even more complex than that of nouns and adjectives. A verb can have hundreds of different forms based on factors such as person, i.e. whether I, you, or he performs an action, or tense, i.e. whether an action is happening in the present, past, or future. Because of the various complexities that are inherent to Greek corpora, it is necessary to normalize the forms of Greek words so that all of the variants are accounted for. This need to normalize is not unique to the Greek language. When working with an English corpus, one might want "am," "is,"  "was," etc., to be identified as forms of the verb "to be." Thus, with respect to this Greek corpus, no matter the inflection of "μῆνις," it should always be identified as the same lexical entity. 

For English corpora, the process of normalization is relatively simple as most English words do not vary widely in their morphology. In the case of nouns, for example, pluralization is their only possible morphological change, and this change affects only their endings. The root, or stem, of a noun usually does not differ between singular and plural forms. Even in the slightly irregular case of "child/children," though the pluralization is not simply the addition of an "s," the stem "child" is unaltered. There are, of course, exceptions to this rule of fixed stems (mouse/mice, goose/geese), but largely this is a constant feature of English words. Even among more mutable parts of speech like verbs, stems largely remain constant in English morphology. As such, there are many programs which "stem" English texts automatically (snowball stemmer is one such program). The result is a version of the original text in which the morphological variants of each individual word are replaced with a single normalized form. For example, a simple stemmer would read the sentence, “The men carried, the women carry, the boy carries, and the girl is carrying,” and replace each form of “carry” with a single, truncated form like “carri.” 

Due to the greater complexity of Greek morphology, such simple stemming processes are inappropriate for managing Greek corpora. The only factors which can change the stem of an English verb are tense and voice (i.e. whether the verb is active or passive). Active verbs are those whose subjects perform the action, whereas passive verbs are those whose subjects are acted upon. For example, the boy actively carries, while the man is passively carried. Tense and voice are the same factors which can change the stem of a Greek verb. Where the two differ, however, lies the number of principal parts and the number of possible inflected forms of those principal parts in each language. Verbs in English have at most three distinct principal parts and they often share the same stem across all three (e.g. carry, carried, carried). In Greek, most verbs have six principal parts, none of which are guaranteed to share the same stem. For example, “φέρω, οἴσω, ἤνεγκον, ἐνήνοκα, ἐνήνεγμαι, ἠνέχθην” are the principal parts of the verb "to carry." Furthermore the number of principal parts is not the only complicating factor for Greek verb morphology. In colloquial English, even an irregular verb such as “to go” has only five possible morphological variants: go, goes, going, went, and gone. Greek verbs, on the other hand, have far more than five possible forms, since each principal part supplies the stem for a myriad of forms that change based on person, number, and mood of the verb. Due to the wealth of diversity within Greek morphology, stemming is not an effective method for normalizing Greek corpora.

These problems can in part be solved by the more advanced process of parsing. This is because, unlike stemmers, a competent parser is able to recognize the lemma, or the dictionary form, of a word and give a detailed report of its exact form, i.e., in the case of a noun, its person, case, and number. Note, this use of "lemma" referring to a word's entry in a dictionary is different from the use of "lemma" to describe the words which identify to what part of the Iliad a scholion refers. As opposed to a stem, which is the root of a word to which endings are added, a lemma is the dictionary form of a word. "Carries" would have a stem of "carri" but a lemma of "carry." Morpheus, a parsing tool created by the Perseus Project, satisfies the requirements for a competent Greek parser. To review, the ultimate purpose of using a parser at this stage in this research is the creation of a text that simplifies the morphological variants of words into single, normalized forms. Thus, using the Morpheus parser allows for the conversion of inflected Greek forms into their respective lemmata. By incorporating Morpheus into a scala script, a text suitable for textual analysis was created by Neel Smith, Melody Wauke, and myself. The created text simply replaced the words of the original text with their respective lemmata. Thus, no matter whether the original text had "μῆνιν" or "μῆνις," a completely parsed version would not differentiate between the two; both would be recorded as "μῆνις". 

Ultimately, the Morpheus parser is able to reduce a large number of the words within the scholia, though there are some words that simply cannot be parsed. Scribal errors, such as misspellings and misplacement of an accent mark, which are not corrected by the scribe himself have no way of being accurately parsed. These will simply appear within the parsed version of the scholia as they appear on the page of the manuscript. The hope is, however, that the number of blatant scribal errors is far fewer than the number of instances where a word can be successfully parsed. In fact, while there are 35,710 unique words within the scholia, 27,116 (75.9%) of the unique words could be successfully parsed by the Morpheus parser. While the fact that 25% of the unique forms within the Venetus A cannot be parsed is a bit discouraging, it is a fact of digital humanities that perfect data is nearly impossible to come by. In that context, 75% is very encouraging result.

When the o-normalized version of the text was created, it was important to consider whether there would be any ambiguity if a Byzantine orthographic variant could be represented by two or more modern equivalents. While this ambiguity ended up being negligent, this same consideration of possible ambiguity was taken up when creating this m-normlaized version of the text. The "m" in "m-normalized" refering to the fact that this version was normalized according to morphological standards. To understand the possible ambiguity in the m-normalized text it is useful to think of similar concerns in English. The word "lay" ca be parsed in multiple ways. Either it is the present form of the verb "lay" or the past form of the verb "lie." This sort of lexical ambiguity happens far more often in Greek, however. To give this assertion some context, of the 27,116 words which could be successfully parsed by Morpheus, 4,909 of these words (18.1%) could be attributed to more than one lemma. These roughly 4,909 instances of ambiguity are much harder to simply ignore than the 75 instances of ambiguity in the Byzantine orthography list. Additionally, the cases of ambiguity caused by multiple lemmata cannot be simply attributed to a difference in accentuation on the final syllable, as was the case for the Byzantine orthographic lists. For many of the words which have multiple lemmata, the lemmata are often share the same root meaning but are different parts of speech. For example, βλάψει could come from either the noun βλάψις, meaning "harm," or the verb βλάπτω, meaning "to harm." In either case it is clear that βλάψει has something to do with harm, but I myself have no way of knowing whether the scholiast intended to use the verb or the noun without the full context on the sentence. Were there only a few examples of ambiguity, it would be a worthwhile process to manually disambiguate the different possibilities. However, the disambiguation of all 4,909 words would be a thesis in itself.

Since the goal of this m-normalized edition is to reduce as much as possible the variant forms of a word to a single word, a tweak was made to our script to make this edition fairly successful, even if not perfect. For example, if the form βλάψεις were to appear in the scholia, it too would share the same double lemmata, βλάψις_βλάπτω_, as the form βλάψει. Thus the two forms,βλάψεις and βλάψει, are still reduced to a single lemma, even if this lemma is imperfect. A better example rests in the case of the two forms ἥ and ὅ which, to the uninitiated eye appear to the unrelated to each other. However, the former could be either the feminine relative pronoun or the feminine definite article. The latter could be, among other things, the masculine relative pronoun or the masculine definite article. As such the two are related and end up sharing the same double-lemmata, ὁ_ὅς_, the former lemma being the definite article and the latter being the relative pronoun. These two forms are exceedingly common words, so the ability to reduce them both to a single form makes dealing with them much easier.

In sum, the final version of the m-normalized text is a much messier dataset than the version of the text that arose from the p-normalized or o-normalized versions of the text. However, the goal of this m-normalized text is not to read it, but rather use it in text analysis. As stated previously, many text analyses require the most common words to be excluded from analysis since they do not add much substance to the content of a text. It just so happens, however, that many of the words which have multiple lemmata are exceedingly common words which would be excluded from analysis anyway. So in the end, the m-normalized version is just another level of text wrangling that can lend itself to certain types of analyses. 

While all four versions of the text (diplomatic, p-normalized, o-normalized, and m-normalized) whose creation was described in this chapter will not be consulted in various forms throughout the rest of the thesis, the different versions of the text certainly have different advantages. If one wanted to analyze *exactly* what is written on a page, then one should consult the diplomatic edition. If one wanted to consult the version without any Byzantine orthographic equivalents, one should use the o-normalized edition. Furthermore, many other versions of the text can be created in order to suit one's needs. In the following chapters, a fifth version of the text, one suited for topic modelling will also be discussed. What is important is that each of these new editions can be appropriately cited both so that they can both be related back to the original edition of the scholia sitting in the Homer Multitext archive, and so that the various editions can be distinguished from one another and be compared.

After this lengthy process of text wrangling, at last there exists a number of editions of the text which can be analyzed through macroanalysis. That distant-reading which Matthew Jockers describes at length in his book can finally be applied to the scholia. The next chapter will detail the specific methods within macroanalysis that were employed to elucidate hidden patterns of language within the scholia. The point to take away from this chapter is that digital corpora are often very messy and it is difficult to prepare them so that they can be sufficiently analyzed. At least for this thesis, an entire semester was spent writing and debugging scala scripts in order to create a usable text for analysis. Even then, there is still more work to be done in order to make the data as clean as possible for future analyses. Since having clean data is one of the most important factor in ensuring that one's results hold any sort of significance, although it may seem like a hassle, this data preparation is the most important step in any digital humanities work.
