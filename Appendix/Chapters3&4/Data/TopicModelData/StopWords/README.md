The files here represent data necessary for the creation of the list of stop words needed  to get accurate data from the topic modelling software. The "topics" of a topic model are nothing more than repeating patterns of co-occurring words. Therefore, frequently-occurring words, like the definite article, occur so frequently that the topic modelling software perceives patterns of co-occurence, i.e. topics, that do not make sense. Thus, the removal of these frequently-occurring words, i.e. topics, from the dataset is absolutely important.

Here, one can see the list of every word in the Venetus A along with how frequently that word occurs. This data is available in the file [MostFrequentWords.tsv](https://github.com/cjschu17/Thesis2016-2017/blob/master/Appendix/Chapters3%264/Data/TopicModelData/StopWords/MostFrequentWords.tsv). This list is based on the morphologically-normalized (m-normalized) version of the scholia fround [here](https://github.com/cjschu17/Thesis2016-2017/blob/master/Appendix/VersionsOfScholia/m-normalized.tsv).

When creating which words should be eliminated from the dataset, I only looked at the 250 most frequently-occurring words. The final list of words eliminated from my dataset in order to create the [version of the text suitable for topic modelling](https://github.com/cjschu17/Thesis2016-2017/blob/master/Appendix/VersionsOfScholia/TM-normalized.tsv) is found in the fie [stopWords.txt](https://github.com/cjschu17/Thesis2016-2017/blob/master/Appendix/Chapters3%264/Data/TopicModelData/StopWords/stopWords.txt).
