##Theory and Methodology

- How the data is originally structured (aka XML)
- Trying to take a text in xml format and analyze it (text only)
- In order to do this, must extract text
- Can't work with raw text (inflected language, BYZORTHO)
- For English, use stemmer; for Greek, use parser
- methods



--------------------





---------------------
For any research such as this, where textual analysis is heavily reliant upon digital technologies, the necessary first step is to make the text of interest machine-readable. For example, a concatenation of every non-white space character in the first line of Poe's "The Raven" can be read by a human, albeit with some difficulty. On the other hand, this string of characters, "`Onceuponamidnightdreary,whileIpondered,weakandweary,`", is read by a computer as just that: a seemingly random collection of characters. Relying on digital techonologies to identify the meaningful word breaks in this example would certainly prove difficult, rendering the above string essentially useless for any textual analysis. While it seems unlikely that one might have such a concatenated text, when texts are being culled from various webpages and outside sources for analysis, the transformation of the text from its original source can often result in an unexpectedly poor and useless format. Thus importing an already-fashioned text requires a fair amount of careful prep work, or text wrangling, in order for it to suit one's analysis.


Luckily, in the case of this work on Iliadic scholia, the text of the scholia has already been written in a format that lends itself easily to digital analysis. This is because, as mentioned previously in chapter 1, the Homer Multitext project has been working for the last nine years to create digital editions of entire Iliad manuscripts. The scholia to the Venetus A manuscript have been transcribed in an XML format with separate editions for each book and type of scholia. By writing in XML, or Extensible Markup Language, the editors of the Homer Multitext project are able not only to record the visible letters on the manuscript page, but also to apply "markup" in order to supply more information about the text. Specifically, the Homer Multitext project follows the guidelines of the Text Encoding Initiative (TEI), a community of digital humanists who have established standards for editing texts. These guidelines inform the members of the Homer Multitext project on how to create digital editions which include additional information that is easily understood by other scholars. This extra information can convey how the scholia are ordered on the page, to which line of the poem a scholion refers, and whether a string of text in the scholion is a quotation from elsewhere in the *Iliad*. These and other more specific TEI elements will be explained as they become relevant to the research of the scholia. However, this section will focus on the structural aspects of the TEI guidelines as they relate to text wrangling.


The Homer Multitext project uses the same basic structural format for editing every scholion. That is, within one TEI element `<div>` there are three parallel subdivisions which each carry information about the scholion. The first subdivision contains just the lemma of the scholion, the second contains a uniform citation for the line of the *Iliad* on which the scholion is commenting, and the final subdivision contains the actual textual content of the scholion itself. Such a logical structure is essential for the creation of a digital scholarly edition. However, this additional structural information hinders textual analysis. While something like `<div type="text"><l>Wrath, sing, oh goddess</l></div>` is a valid and logical XML transcription of the firt few words of the *Iliad*, clearly text like `<div>` and `<l>` should not be included in an analysis. If one were working with a small amount of scholia, it may make sense just to manually extract all the text, excluding the lemmata and citations. The 8,000 scholia of this dataset, however, renders manual extraction impractical. In order to avoid this labor-intensive process, we wrote a program using scala, a computer programming language, which was able to perform this extraction automatically.


Even with a complete collection of words from all the scholia, the dataset would still be insufficient for any serious text analysis if left unaltered. The words in this collection have been transcribed exactly as they appear on the manuscript page. This means that any  orthographic variants, scribal errors, and even leftover markup from the editing process can clutter up the collection. In regard to the leftover markup, it can easily be extracted from the text of the scholia. Since the markup is written in English and the scholia are written in Greek, it is fairly simple to filter out the letters of the English alphabet from the surrounding Greek. The problems which scribal errors and orthographic variants pose for data analysis are more complex and will receive more attention in the later discussion concerning the parsing of a text. For now, just an overview of what is meant by scribal errors and orthographic variants is required. The difference between these two is subtle, but important. Both refer to the misrepresentation of a word based on modern standards of Greek orthography. Put simply, a scribal error refers to the incorrect spelling or incorrect placement of a diacritical mark while an orthographic variant refers to the strict absence of an expected diacrtical mark. Ultimately scribal errors are words rendered incorrectly. Conversely, orthographic variants are equally valid forms of words, but do not follow modern orthographic standards. Instead, they follow standards which were accepted at the time of writing. For example, a modern reader would expect the ancient Greek word for "wrath" to be written as μῆνις. If the scholiast were to add an additional letter to the word (μῆννις) or if he were to change the accentuation (μὴνις), these mutations would alter the very identity of the word and thus be considered scribal errors. On the other hand, if the scholiast were to leave out the accent mark from an otherwise correctly spelled word (μηνις), the lexical identity remains intact. Thus, for our purposes, "μῆνις" is orthographically equivalent to the unaccented "μηνις". How exactly such noise is handled within this corpus of Greek scholia will be discussed later, but it should be clear that the presence of alternative forms for the same word poses difficulties for textual analysis.

The inflected nature of the Greek language poses even more such difficulties for analysis. The forms of Greek words change depending on their function within a sentence. Thus, the iota-sigma ending of "μῆνις" indicates that it is most likely the subject of a sentence or clause. "μῆνιν" still means wrath, however the iota-nu ending indicates that it is most likely the direct object of some verb. The inflection of verbs is even more complex than that of nouns and adjectives. A verb can have hundreds of different forms based on factors such as person, i.e. whether I, you, or he performs the action, or tense, i.e. whether an action is happening in the present, past, or future. Because of the various complexities that are inherent to this dataset, it is necessary to normalize the forms of words so that all of the variants are accounted for. This need to normalize is not unique to Greek corpora. When working with an English corpus, one would want "am," "is,"  "was," etc., to be identified as a form of the verb "to be." Thus, with respect to the Greek corpus, no matter the inflection or accentuation of "μῆνις," it should always be identified as the same lexical entity.


For English corpora, the process of normalization is relatively simple as most English words do not vary widely in their morphology. In the case of nouns, for example, pluralization is their only possible morphological change, and this change affects only their endings. The root, or stem, of a noun usually does not differ between singular and plural forms. Even in the slightly irregular case of "child/children," though the pluralization is not simply the addition of an "s," the stem "child" is unaltered. There are, of course, exceptions to this rule of fixed stems (mouse/mice, goose/geese), but largely this is a constant feature of English words. Even among more mutable parts of speech like verbs, stems largely remain constant in English morphology. As such, there are many programs which "stem" English corpora automatically (snowball stemmer is one such program). The result is a version of the original text in which the morphological variants of each individual word are replaced with a single normalized form. For example, a simple stemmer would read in the sentence, “The men carried, the women carry, the boy carries, and the girl is carrying,” and replace each form of “carry” with a single, truncated form like “carri.” 

Due to the greater complexity of Greek morphology, such simple stemming processes are inappropriate for managing Greek corpora. As stated previously, the stem of a Greek word changes considerably depending on tense, for example. As a result, most verbs have six principal parts, none of which are guaranteed to share the same stem. For example, “φέρω, οἴσω, ἤνεγκον, ἐνήνοκα, ἐνήνεγμαι, ἠνέχθην” are the principal parts of the verb "to carry." Verbs in English, on the other hand, have at most three distinct principal parts and often share the same stem across all three (e.g. carry, carried, carried). Furthermore the number of principal parts is the not the only the complicating factor for Greek verb morphology. In English, even an irregular verb such as “to go” has only five possible morphological variants: go, goes, going, went, and gone. So, not only does a Greek verb have more principal parts than an English verb has possible forms, but, to reiterate, each principal part of a Greek verb can itself have myriad possible morphological variants. Thus, stemming is not an effective method for normalizing Greek corpora as it is for English corpora.

